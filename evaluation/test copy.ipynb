{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load  cais/mmlu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/news_factor.csv')\n",
    "\n",
    "print(data.loc[0, 'full_prefix'])\n",
    "print(data.loc[0, 'completion'])\n",
    "print(data.loc[0, 'contradiction_0'])\n",
    "print(data.loc[0, 'contradiction_1'])\n",
    "print(data.loc[0, 'contradiction_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# load data\n",
    "def extract_example(row):\n",
    "    return {'full_prefix': row.full_prefix, 'completion': row.completion,\n",
    "            'contradictions': [row.contradiction_0, row.contradiction_1, row.contradiction_2]}\n",
    "\n",
    "\n",
    "def read_data(path, prefix_col):\n",
    "    df = pd.read_csv(path)[[prefix_col, 'doc_id', 'completion', 'contradiction_0', 'contradiction_1', 'contradiction_2']]\n",
    "    df.rename(columns={prefix_col: 'full_prefix'}, inplace=True)\n",
    "    return df.apply(lambda row: extract_example(row), axis=1).to_list()\n",
    "\n",
    "# load model\n",
    "def load_tokenizer(model_name, max_tokens):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='right', truncation_side='left',\n",
    "                                              model_max_length=max_tokens)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(model_name, cache_dir=None, max_tokens=1024):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    multi_gpus = torch.cuda.device_count() > 1\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    model_args = {}\n",
    "    if cache_dir is not None and device != 'cpu':\n",
    "        model_args[\"cache_dir\"] = cache_dir\n",
    "    if multi_gpus:\n",
    "        model_args[\"device_map\"] = \"auto\"\n",
    "        model_args[\"low_cpu_mem_usage\"] = True\n",
    "    if hasattr(config, \"torch_dtype\") and config.torch_dtype is not None:\n",
    "        model_args[\"torch_dtype\"] = config.torch_dtype\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, **model_args).eval()\n",
    "    if not multi_gpus:\n",
    "        model = model.to(device)\n",
    "    tokenizer = load_tokenizer(model_name, max_tokens)\n",
    "    print(model.dtype)\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    return model, tokenizer, device\n",
    "\n",
    "# prepare examples for evaluation\n",
    "def format_data(ex):\n",
    "    prefix = ex['full_prefix']\n",
    "    completion = ex['completion']\n",
    "    contradictions = ex['contradictions']\n",
    "\n",
    "    # make sure completion don't contain trailing spaces\n",
    "    completion = completion.lstrip(' ')\n",
    "    contradictions = [cont.lstrip(' ') for cont in contradictions]\n",
    "\n",
    "    # if the prefix ends with a new line, just concatenate.\n",
    "    # Else, add space to the completion, remove it from the prefix if necessary\n",
    "    if prefix.endswith(' '):\n",
    "        prefix = prefix[:-1]\n",
    "        batch = [f\"{prefix} {completion}\"] + [f\"{prefix} {cont}\" for cont in contradictions]\n",
    "        labels_batch = [f\" {completion}\"] + [f\" {cont}\" for cont in contradictions]\n",
    "    else:\n",
    "        batch = [f\"{prefix}{completion}\"] + [f\"{prefix}{cont}\" for cont in contradictions]\n",
    "        labels_batch = [completion] + contradictions\n",
    "    return batch, labels_batch\n",
    "\n",
    "\n",
    "def prep_batch(ex, tokenizer, device):\n",
    "    # prepare examples for tokenization\n",
    "    batch, labels_batch = format_data(ex)\n",
    "    # encode full text (context + completions)\n",
    "    encoding = tokenizer(batch, padding=True, truncation=True, return_tensors='pt', add_special_tokens=False).to(device)\n",
    "    encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "    input_ids = encoding['input_ids']\n",
    "    # extract labels from input text\n",
    "    labels_encoding = tokenizer(labels_batch, padding=True, truncation=True, return_tensors='pt', add_special_tokens=False).to(device)\n",
    "    input_lens = torch.sum(encoding['attention_mask'], axis=-1).to(device)\n",
    "    target_lens = torch.sum(labels_encoding['attention_mask'], axis=-1).to(device)\n",
    "    offsets = input_lens - target_lens\n",
    "    positions = torch.arange(0, encoding['input_ids'].size(-1))[None, :].to(device)\n",
    "    labels_mask = (positions >= offsets[:, None]) * encoding['attention_mask']\n",
    "\n",
    "    labels = input_ids*labels_mask + (-100)*(1-labels_mask)\n",
    "\n",
    "    # assert all labels match\n",
    "    for input_id, label, target_len, offset, comp in zip(input_ids, labels, target_lens, offsets, labels_batch):\n",
    "        assert torch.all(input_id[offset: offset + target_len].eq(label[offset:offset+target_len])), \"labels don't appear in input ids\"\n",
    "        assert torch.all(label[:offset] == -100), \"labels include redundant prefix\"\n",
    "        assert torch.all(label[offset + target_len:] == -100), \"labels include redundant suffix\"\n",
    "    encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "    return encoding, labels, target_lens\n",
    "\n",
    "\n",
    "def get_losses(logits, labels):\n",
    "    loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "    nll = loss_fct(logits.reshape(-1, logits.size(-1)), labels.reshape(-1)).cpu()\n",
    "    nll = nll.view(labels.size())\n",
    "    return nll\n",
    "\n",
    "\n",
    "def run_eval(model, tokenizer, data, device):\n",
    "    all_scores = torch.empty((len(data), 4))\n",
    "    for i, ex in tqdm(enumerate(data)):\n",
    "        print(ex)\n",
    "        break\n",
    "        input_ids, target, target_lens = prep_batch(ex, tokenizer, device=device)\n",
    "        with torch.no_grad():\n",
    "            out = model(**input_ids)\n",
    "            nll = get_losses(out.logits[..., :-1, :], target[:, 1:])\n",
    "\n",
    "        # get scores for the full the sequence\n",
    "        scores = torch.sum(nll, axis=-1)\n",
    "        scores = scores / target_lens.to('cpu')\n",
    "        all_scores[i] = scores\n",
    "        if i % 100 == 0:\n",
    "            acc = np.sum(np.argmin(np.array(all_scores[:(i+1), :].tolist()), axis=1) == 0) / (i+1)\n",
    "            print(f\"processed: {i+1}/{len(data)} examples. accuracy: {acc}\")\n",
    "    return all_scores\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    prefix_col = 'turncated_prefixes'\n",
    "    data = read_data(args.data_file, prefix_col)\n",
    "    model, tokenizer, device = load_model_and_tokenizer(args.model_name, args.cache_dir, max_tokens=args.max_tokens)\n",
    "    all_scores = run_eval(model, tokenizer, data, device)\n",
    "    data = pd.DataFrame(data)\n",
    "    data['scores'] = list(all_scores.to('cpu').numpy())\n",
    "    acc = np.sum(np.argmin(np.array(data['scores'].to_list()), axis=1) == 0) / len(data)\n",
    "    print(f\"acc = {acc}\")\n",
    "    data.to_json(get_results_path(args.output_folder, args.model_name), lines=True,\n",
    "                 orient='records')\n",
    "    print(\"Done!\")\n",
    "\n",
    "\n",
    "def get_results_path(output_folder, model_name):\n",
    "    return os.path.join(output_folder, model_name.split('/')[-1] + '.jsonl')\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser()\n",
    "\n",
    "#     # Data params\n",
    "#     parser.add_argument('--data_file', required=True, type=str, help=\"csv file\")\n",
    "#     parser.add_argument('--output_folder', required=True, type=str)\n",
    "\n",
    "#     # Model params\n",
    "#     parser.add_argument('--model_name', default='gpt2', type=str)\n",
    "#     parser.add_argument('--max_tokens', type=int, default=1024)\n",
    "\n",
    "#     parser.add_argument(\"--cache_dir\", type=str, default=\"/dev/shm/cache-transformers/\")\n",
    "#     args = parser.parse_args()\n",
    "#     main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_col = 'turncated_prefixes'\n",
    "data = read_data('data/news_factor.csv', prefix_col)\n",
    "model, tokenizer, device = load_model_and_tokenizer('gpt2', './cache-transformers/', max_tokens=1024)\n",
    "all_scores = run_eval(model, tokenizer, data, device)\n",
    "data = pd.DataFrame(data)\n",
    "data['scores'] = list(all_scores.to('cpu').numpy())\n",
    "acc = np.sum(np.argmin(np.array(data['scores'].to_list()), axis=1) == 0) / len(data)\n",
    "print(f\"acc = {acc}\")\n",
    "data.to_json(get_results_path(args.output_folder, args.model_name), lines=True,\n",
    "                orient='records')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_batch(data[0], tokenizer, device)\n",
    "# format_data(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"parquet\", data_files=\"/iopsstor/scratch/cscs/dfan/data/robots-txt/RawData-NYTimes/*.parquet\")\n",
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "# Set the folder containing the .txt files\n",
    "folder_path = \"/iopsstor/scratch/cscs/ansaripo/factor/NYT articles/new_articles\"\n",
    "dataset_name = \"nytimes_new_verbatim_256\"\n",
    "pre_len = 256\n",
    "\n",
    "# Read text files into a list of dicts\n",
    "data = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(folder_path, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "            data.append({\"filename\": filename, \"text\": f.read()})\n",
    "\n",
    "# Create a Hugging Face Dataset\n",
    "dataset = Dataset.from_list(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pandas()['text'].apply(lambda x: len(x.split())).hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('alehc/swissai-tokenizer')\n",
    "for data in dataset:\n",
    "    if len(data['text'].split()) <= 256:\n",
    "        print('found')\n",
    "        print(data['text'])\n",
    "        print(len(tokenizer.tokenize(data['text'])))\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]\n",
    "pd.cut(dataset['train'].to_pandas().loc[:1000, 'text'].apply(lambda x: len(x.split())), bins=bins, include_lowest=True).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# get the first 20000 examples\n",
    "# fset = dataset[\"train\"].select(range(12800))\n",
    "fset = dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained('alehc/swissai-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# # get the first 20000 examples\n",
    "# subset = dataset[\"train\"].select(range(12800))\n",
    "# tokenizer = AutoTokenizer.from_pretrained('alehc/swissai-tokenizer')\n",
    "\n",
    "def split_example(examples, input_size=pre_len, max_tokens=4096):\n",
    "    # text = example['text']\n",
    "    # tokens = tokenizer.tokenize(text)\n",
    "    # input = tokenizer.convert_tokens_to_string(tokens[:input_size])\n",
    "    # target = tokenizer.convert_tokens_to_string(tokens[input_size:max_tokens])\n",
    "    # return {'input_text': input, 'target_text': target}\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    \n",
    "    for i, text in enumerate(examples['text']):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        input = tokenizer.convert_tokens_to_string(tokens[:input_size])\n",
    "        target = tokenizer.convert_tokens_to_string(tokens[input_size:max_tokens])\n",
    "        input_texts.append(input)\n",
    "        target_texts.append(target)\n",
    "    \n",
    "    return {'input_text': input_texts, 'target_text': target_texts}\n",
    "\n",
    "subset = fset.map(split_example, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_examples = subset.filter(lambda example: len(example['target_text']) != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(positive_examples), len(subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the dataset\n",
    "# positive_examples.save_to_disk('/iopsstor/scratch/cscs/ansaripo/data/nytimes_verbatim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#login to huggingface\n",
    "!transform\n",
    "!transformers-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push the dataset to the hub\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "HF_TOKEN=''\n",
    "from huggingface_hub import login\n",
    "login(HF_TOKEN)\n",
    "DatasetDict({'test': positive_examples}).push_to_hub(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "!export HF_TOKEN=\n",
    "model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-Nemo-Base-2407')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset(\"mansaripo/nytimes_verbatim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mansaripo/nytimes_verbatim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataset['test']:\n",
    "    if '00 photographs.' in data['target_text']:\n",
    "        print(data['input_text'])\n",
    "        print(data['target_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataset['test']:\n",
    "    if 'why?' in data['target_text'] and len(data['target_text']) < 100:\n",
    "        print(data['input_text'])\n",
    "        print(data['target_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "output_file = \"/users/ansaripo/deepseek_questions_mcq.json\"\n",
    "dataset_name = \"nytimes_mcq_with_context\"\n",
    "pre_questions = json.load(open(output_file, \"r\")) if os.path.exists(output_file) else []\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('alehc/swissai-tokenizer')\n",
    "\n",
    "\n",
    "# tokens = tokenizer.tokenize(text)\n",
    "# input = tokenizer.convert_tokens_to_string(tokens[:input_size])\n",
    "# target = tokenizer.convert_tokens_to_string(tokens[input_size:max_tokens])\n",
    "# input_texts.append(input)\n",
    "# target_texts.append(target)\n",
    "\n",
    "article_data = \"/iopsstor/scratch/cscs/dfan/data/robots-txt/RawData-NYTimes/*.parquet\"\n",
    "processed_data = load_dataset(\"parquet\", data_files=article_data)['train']\n",
    "\n",
    "for q in pre_questions:\n",
    "    tokens = tokenizer.tokenize(processed_data[q['index']]['text'])\n",
    "    input = tokenizer.convert_tokens_to_string(tokens[:(4096 - 70)])\n",
    "    q['generated_question']['article'] = input\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "DatasetDict({'test': Dataset.from_list([q['generated_question'] for q in pre_questions])}).push_to_hub(dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset(\"mansaripo/nytimes_mcq_with_context\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HOME=/iopsstor/scratch/cscs/ansaripo/huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $HF_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "datsetset = load_dataset(\"mansaripo/nytimes_mcq_eval\")\n",
    "\n",
    "correct = 0\n",
    "for data in datsetset['test']:\n",
    "    answer = data['generated_question']['answer']\n",
    "    pred = data['prediction']\n",
    "    if answer == pred:\n",
    "        correct += 1\n",
    "print(f\"Accuracy: {correct/1000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "datsetset = load_dataset(\"mansaripo/nytimes_mcq_eval_blind\")\n",
    "\n",
    "correct = 0\n",
    "for data in datsetset['test']:\n",
    "    answer = data['generated_question']['answer']\n",
    "    pred = data['prediction']\n",
    "    if answer == pred:\n",
    "        correct += 1\n",
    "print(f\"Accuracy: {correct/1000}\")\n",
    "print(f\"Accuracy: {correct/len(datsetset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "datsetset = load_dataset(\"mansaripo/nytimes_mcq_eval_blind_gpt\")\n",
    "\n",
    "correct = 0\n",
    "for data in datsetset['test']:\n",
    "    answer = data['generated_question']['answer']\n",
    "    pred = data['prediction']\n",
    "    if answer == pred:\n",
    "        correct += 1\n",
    "print(f\"Accuracy: {correct/len(datsetset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
